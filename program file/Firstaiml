linear regression : statistical algorithm that model relationship between dependent and independent variables. linear regression finds best-fitting linear equation predicts dependent or target variables
Ordinary least squares most common method used in linear regression aim to minimize error blw observed data point and predicted values(Ridge regression-imporve performance of lr
, Lasso regression->increase model interpretation, Elastic net combines ridge and lasso, Non negative least squares regression variant-> making ideal for Non negative values such as price or temp

Logistic regression: Binary classification problems. refer class or category, relationship blw independent variables and probability of observation belonging to particular class.it applies a user defined threshold(hyperparameter)
Logistic used for decision making data into two classes.such as whether to invest in certain stock,buy or sell a 
business or make medical diagnosis.



Decision trees :provides way make systematic decisions using flowchart like set binary decisions. It is hierarchical,branching tree like structure 
if condition1 and condition2 ... and condition3 then outcome
Decision trees used both classification and regression tasks. making decission at each node
starting with labeled data and potential features for branching. divided into subsets hold specific feature sets. 
ex-gini metric lower gini value higher quality.
limitation DT : can overly complex, leading to overfitting. 
overfitting can controlled through methods such as tree pruning, setting a minimum sample count for leaf nodes or limiting the tree's depth

Support vecotor machines(SVMs): a supervised ml algo use classification and regression task.operates on principle of finding a hyperplane or line in two -dimension space that separates two classes of data points.
discriminative classifier solve both classification and regression problems as supervised algorithm, svm requires labeled data, model train on subset data 
SVM classifier are well known for their abiltiy to identify hand-written digits
use for facial recoginition, image segmentation(differentiating organs or tssues in MRI) life expectancy, and expected return on investment(ROI) in financial industry
Linear SVM: main objective to find line or plane that best separates classes in target variable from each other.
Kernel SVM: Many real world problem cannot divided into classes by simple line.svm applied using "kernel trick" uses curved lines insted of strainght lines to separate the class.iris use linear kernel and linearsvm
SVM can high accuracy, working well with high-dimensional data, low sensitivity outliers.

K-nearest neighbors(KNN) : train data based on label target classes. algo can conceivably use lidar images distinguish 3D human and 2D pictures
KNN- prevalent in many sectors, such as healthcare(for disease prediction), finance(credit scoring), and e-commerce(for rcommendation systems)
KNN classification (predict categories): uses knn algotithm to categorize labeled data point by analyzing the categories labeled data point by analyzing categories of K nearest neighbors in feature space 
KNN classification has seen widespred use in image recognition where image classified based on feature of similar images in dataset.
voice recoginition system use knn to classify voice command by comparing them to known voice patterns
KNN classify genes into dfferent functional cateories in bilogical sciences and genetics.

KNN regression: class becomes very large(continous value)
real estate: predict price of house based on feature(area, nu bedroom, location) knn reggression used averageing the prices of Kmost similar house
stock market forcasting: historical stock price movements can inform future price predictions
Weithted averaging: closer neighbors can give more weight in averaging process


EValuation matrics for classification
Accuracy: straightforward metric represent ratio of correctly predcted instances total instances in dataset. beneficail when class distribution is blanced 
in case class is imbalance accuracy can misleading for instance, 95% samples belong to classA and 5% belong to classB model predicting everything as classA still accuracy 95%

Precision and recall: metric measures accuracy of positive prediction wile recall measures the ration of positive instance 
precision and recall especially important when false positive and false negatives carry different costs.for flase negative(disease present but not detected) more dangerous in medical test than false positive(disease absent but indicated as present)

Confusion matrix : is table used to describe performance of classification model on set of data true values known four value TP,FP,TN,FN 
true positives and true negatives indicate correct classifications, where false postivies and false negatives indicate errors, ture positive would disease present and detected while true negative would be disease absent and not detected as present


Area Under the curve-receiver operating characteristic(AUC-ROC): curve plots true positive rate(recall) against false positive rate
AUC represent area under ROC give scalar value, indicates model ability to distinguish between positive and negtive class
AUC-ROC useful dealing with dataset with class imabalance. AUC value 0.5 suggests no discrimination(equivalent to random guessing) 
value 1 indicates perfect discrimination. ex- credit card transaction most lgitimate very few fraudulent, which represents imabalance two transaction classes.


Evaluation matrics for regression: examine relation blw two or more variable of interest.
R-squared(R2): coeffieient determination, quantifies proportion of variance in dependent variable that predictable from independent variables 
range(0-1) 1 INDICates regression predictions perfectly fit data R2 suggest better fit, doesn't always mean better model, especially if model is overfitting.
Mean squared error(MSE): av squared differences blw observed and predicted values.MSE emphasize more significant errors over smaller ones
since squares residuals.means sensitive to outliers lower mse generally consider better
Mean absolute error(MAE): calcualte avg absolute difference blw observed and predicted values
MAE treates all errors equally, so it is less sensitive to outliers compared to Mse similar to MSE a lower MAE indicates better model fit data



Natuaral Language Processing (NLP):bridge blw intricate nuances of human language and analytical prowess data science.
NLP use AI focuses on teaching computers to understand,intrpret, and interact with human language way that both meaningful and useful. 
Computer science: programmer design and develop NLP algotithms to efficeintly process and analyze vast volumes of text data.these algo enable machine to perform text classification, sentimetn analysis and Language translation task
Liguistics: linguistics, study of languages, brings valuable insights to NLP , programmers developers use these insights to help NLP system understand and work with text and speech data
Machine learning: programmer use ml to train deep learning models to learn and adapt language patterns.NLP-driven model have signigicantly advanced text understanding and response
Data science: collecting, analyzing preprocessing, and structuring vast amount ot textual dat for analysis. NLP leverages data science technique to train model derive insights from textdat

Practical application and task NLP
Text understanding: NLP systems use text understanding analyze and intrpret meaning of textual data.
Language translation: NLP use convert text or speech from one language to another. NLP powwered translation system apply advanced techniques to ensure accuracy and fluency translated content, helping people from different linguistic backgrounds communicate.
Sentiment analysis: application of NLP help business analyze customer sentiment and opinion expressed text data such as social media post, review or comments.allow businesses to understand whether customer express positive negative aor neutral sentiment
Speech recognition: NLP converts spoken language into text, process widely used voice assistants and transcription services. NLP also improves accessibility individuals with disabliltes


two spect NLP 
Natural language understanding(NLU):enable computer to understand interpret human language.It involves task as text comprehension, language modeling,sentiment analysis and recoginition. NLU allow NLP systems to grasp meaning, context  and behind text or speech
Natural language generation(NLG): focuses on generation of human like text or speech by computers. NLG systems transform structuted data into coherent and contextually relevant languages.
this use various application including text summarization, language translation, chatbots, voice assistants, and personalized email to enable machine to speak to humans and natuarally and understandably

Common programs using NLP
Microsoft word: nlp feature like spell check, grammar check, and autocorrection. It also offers suggestion for improved sentence structure and readabilty
Google search: engine use NLP algorithm to understand user queries and provide relevant search results. Feature like google autocomplete and featured snippets also use nLP
social media platforms: like facebook and linkedin use NLP for content recommendation, sentiment analysis and trending topic identification.
voice assistants:like apple siri,amazon's alexa, google assistant rely on NLP understand and respond to spoken language commmand and questions.
Customer support chatbots: service use chatbot nLP capabilities provide instant customer support, answer frequently asked questions and assist users with inquiries.
Content recommendation systems: Streaming platforms like Netflix and music service like spotify use NLP for content recommendation, suggesting movies, shows, or music base on preferences
Healthcare information systems: NLP usd heathcare information system process medical records extract patient information and assist in clinical decision
Virtual learning environments: automated grading ,personalized course recomendation nlp.

Ambiguity: ambiguous, word and phrases have multiple meaning and association depending on contect how the word is used. NLP models use contextual information and surrounding words to disambguate.
Lack of context: struggle with maintaining context in long conversation or understanding subtle nuancess. Models with memory mechanisms, like transformers, help capture context over longer text
Bias and fairness: model inherit biases from training data, leading to biased or unfair outcomes.Addressing bias requries diverse and representative training data,ongoing monitoring, fairness-aware model design mitigate and correct biases.


Tool and techniques include NLP
Tokenization: Breaks text into individual words or tokens,forming basis for most NLP tasks.
Stemming and lemmatization: words to root forms,aiding text analysis and rducing vocabulary variations.
Part of speech tagging(POS): grammatical labels to words,helping syntax analysis and understanding sentence structure
Named entity recognition(NER): categorize named entities,such as names people,place and businesses in text.
Sentiment analysis: tool assesses emotional tone expressed text and often used for sentiment classification,such as positive,negative or neutral.
Text classification:text into predefined classses or labels,such as spam detection or topic classification
Machine translation:like google translate,NLP techniques translate text blw languages.
Pre-trained language model:Large scale such as google BERT and openAI chatGPT, revolutionized NLP providing deep contectual understanding and generating human like text.


NLP libraries and framework
NLTK : comprehensice python library for NLP, contain many tool and resources task like tokenization,stemming,lemmatization,pos tagging.
spaCy: fast and efficient python library for NLP. provids pre-trained models and supporttask like tokenization,pos tagging, and NER.spaCy known for speed and production-readiness.
Transformers from Hugging Face:Provides state-of-the-art  pre-trained language models shuch as google BERT.It's widely used various NLP task text classification,language generation , translation
Standford NLP:tool and model including tokenization,pos tagging and NER.It known for its high quality models and accuracy ofeten used for academic and research purposes.
AllenNLP: deep learing framework specifically designed for NLP research. provides pre-built components for building and evaluting NLP models

Text acquisition->Text preprocessing_>sentence structure and interpretation_>Numerical representaion->Machine training and accuracy checks->Application development


Tokenization,stemming,lemmatization,pos tagging,NER, sentiment analysis,text classification,machine translation 
NLTK,spaCy,transformers from Hugging Face,Standford NLP ALLenNLP


UNSUPERVISED MACHINE LEARNING:
unsupervised ml similar to learing without a teacher, computer learn exploring data and finding structre and patterns on its own ex spot patteren in customer data
Unlabeled data:don't have known or specified outcome.
usupervised learning: unlabel data uncover hidden patterns or structures in data, unlabeled data makes it valuable in real world scenariosex-text,image,transaction record
it delineate inherent structures,regularities, or recurring thimes within data. these underlying patterns offer valuagle insight knowledge,data compression,subsequent supervised tasks.

Clusters: group similar each other according defined similarity measure ex-
Association learing focus on uncovering patterns or relationships blw seemingly unrelated item occurrences.
Reduced dimensionality:smaller features often extracted from dataset preserves essential information content data reduce number dimension and amount of data

Clustering: such K-means, hierarchical clustering, DBSCAN aim to divide data into distincet groups based similarity.
Dimensionality reduction: as PCA, t-SNE , autoencoders can undcover lower dimensional structures within high dimensional data salient features or patterns.
Association learning: as Apriori or FP-growth used discover items often associated with each other dataset
Anomaly detection: recoginizing normal patterns in data,unsupervised method spot outliers or anomalies deviate from patterns

Advantange unsupervised learning:
Cost effective and scalable: large amount of unlabeled data than labeled data.leverage large data sets extract patterns and insights
Revealing unknown patterns:not being constrained by predefined labels,unsupervise learning unexpected or novel patterns that might not be apparent or considerad
Data preprocessing: recognizing and extracting patterns help cleaning data,imputing missing values,or transformin features 
Avoid bias: no labels guide learning process, less risk of bias form labeling process however,biases data collection or representatin can still impact unsupervise models

two individuals with close ages and income would be considered similar likely grouped into same cluster comparing driving distance ciy manhattan distance metric
usupervised learnig provide quantitative measure to compare data point, enabling algorithm to discover patterns group or reduce dimension dataset gidance of explicit labels

Handling-dimensional data features researchers enconter curse dimensionality.
Curse dimensionality: arise working with high-dimensional data. as number of dimensions(or features) dataset increases, volume of data required perform ml task increase exponentially.

Dimensionality reduction: techniques such as principal component analysis(PCA),t-SNE and autoencoders help reduce number of dimensions while preserving much of data structure as possible
Feature selection:instead using all available features, algorithms or domain expertise employed to select subset most informative and non-redundant feture
Sampling: curse due to overwhelmingly large number of samples across dimensions, techniques such as random sampling or more sophisticated sampling method help reduce computational
Regularization: Techiniques applied penalize complexity of model, making less likely to overfit high dimensional noise.
Using specialized algorithms: algo are designed be adapted work better with high-dimensional data, such random forests or certain kernel methods clustering.

method -clustering method operate different way might partition data into subsets, establish hierarchical trees, or determine clusters based on density.
Dimensionality reduction projection onto lower dimensional space while preserving data.

Scikit-learn :
python library for ml and data analysis. Born from scientific computing ecosystem around pyhton.
It offer array of tools that make process of modeling data simple and efficient. known for user friendly interface, scikit-learn encompasses comprehensive set of algorithms for various task
including dimensionality reduciton
open-source and community-driven; scikit-learn is open source licensed under BSD, developement benifited from the contribution numerous indibidual
Built on NumPy,SciPY and Matplotlib: At core scikt-learn heavily relies on NumPy numerical operation, SciPy certain mathematical functionalities, oftern pair with Matplotlib for visualization
Consistent API:Scikit-learn defining feature consisten API. Regardless type algotithm be classification, regression, clustering model instantiation, training, prediction 
Wide array algorithms: from classic statistical model to more modern ml algo, scikit-learn boasts comprehensive toolkit. 
unsupervised include clustering (KMeans,Hierarchical clustering),dimensionality reduction(PCA,t-SNE), even manifold
Extensibility: Scikt-learn has vast suite method also designed allow users to seamlessly develop and integrate their custom algorithms
Preprocessin and utilities: Scikit-learn provides range utilities for data preprocessing,including tool feature scaling,encoding categorical variables, imputation
Model evaluation and hperparameter tuning: Scikti-learn tool excel for model evaluation and hyperparameter tuning. include cross-validation, grid search, various metrics evaluate model per
Pipline integration:power full features ability create pipeline. ability streamlines many steps model building process, ensuring transformations and estimator executed in correct sequence
Sparse data support: scenarios where data is primarily zeros(like text data represented bag of words format), Scikit-learn can efficiently handle sparse matrices, making more memory efficient.'
Interoperablility: Scikit-learn not handle deep learning tasks, plays well with other libraries. Libraries such as Keras, example integrated with scikit-learn for more complex tasks.


Clustering: data mining technique for grouping unlabeled data based on similarities or difference

K-means clustering:popular unsupervised clustering technique in K-mean clustering,'K' number of user desired clusters for algorithm form from dataset. centroid point represents center each cluster reference point determining distance blw
data pont cluster center first centroid slected randomly fromdata 

Unsupervised KNN: it supervised learning algorithm, can use it unsupervised learning. Unsupervised KNN algo groups togethere data point based on similarity their KNN feature space. idea is if most of your immediate neithbors belong paritcular cluster
DBScan: density-based spatial clustering application with noise. DBScan group point closely packed together based on distance measure and minimum number of point. useful finding clusters of arbitrary shapes instead of Kneasn, which tends to find spherical clusters.
Point in low density regions,neighboring point are too sparse,typically considerd noise or border points

Hierarchical clustering: create tree of clusters. It can either agglomerative(bottom-up approach) or divisive(top-down approach)
in agglomerative hierarchical clustering, each data point starts as its own cluster,and pairs of clusters merge as one moves up hierarchy.
Divisive clustering starts with one cluster,performing split recursively as one moves down the hierarch result dendrogram- tree like diagram

HDBScan: hierarchical density-based spatial clustering of application with noise. extension of DBScan stand out by ability find clusters of varying densities. 
insted using single value density(DBScan),HDBScan constructs hierarchy or clusters dynamically determines best density value cluster formation, making more flexible capable of detecting nested or compounded clusters.


Dimensionality reduction:tackle challenge transforming original high-dimensional data into lower dimensional space,intending keep much data possible unchanged.
Ex 2D,3D,1D
2D reduction mthods (linear projection, Manifold projection)

linear projection: subset of dimensionality reduction techniques transform data onto lower dimensional subspace using lingear combinations original feature.

Principal component analysis: PCA use dimensionality reduction,identifies axes(principle components) in data maximize variance.
by projecting data onto axes,PCA retains data's most significant structures and pattern while reducing dimensions.resulting principal components are othogonal each other ensuring capture non redundant information

Independent component analysis(ICA): PCA look for uncorrelated axes independent component analysis(ICA) aims to find statistically independent axes.particularly useful when goal separate multivariate signal additive,independent components, makine popular 
in audio and image processing fiels.ICA often used task such as blind source separation,objective extract individual signals from mix, separating multiple voice speaking simultaneously

Singular value decomposition(SVD): factorization method decomposes matix into three other matrices, capturing essence original data.
in context of dimensionality reduction, SVD trrasnform data into space defined by singular values, which are indicative data inherent structures.
SVD underlying technique behind many other mthods, including PCA and applied areas such as recommendation system in NLP


Manifold projection: finding linear combination of features reduce dimensions, minifold projection method design capture nonlinear structure inherent data. 
They attempt to map high dimensional data to lower dimensional space, maintainging realtive distances blw points,especially close together. idea is preserve geometric and topological properities data manifold offering more intricate sometimes more accurate representation of original data.


T-SNNE(t-distributed stochastic neighbor embedding): popular manifold learnig algorithm that visulize high dimensional data in two or three dimensions. By convering Euclidean distances blw data point conditonal probabilities,t-sne captures both 
local and global structures. it t-distiributed probability measure ensures similar data point modeled by nearby point low dimensional space, making especially 
effective for clustering visualizaiton. can comptationaly intensive and might not always yield consisten results across different runs.


UMAP(uniform mainifold approcimation and projction):
gian popularity for its efficeinecy and versatility.like t-SNE, UMAP focus preserving local structure,
leading consistent interpretable embeddings.foundation toplogical data analysis give robust mathematical grounding, outperforms other techniques in terms of runtime, especially on large data

Combined dimensinlity reduction and clustering:
by applying dimension reduction first potentially rejuvenate performance of clustering algorithm. two step approach- first simplifying data structue then clustering often enhance quality of cluster making more cohesive and bettere separated. t
this combination not only sidestep pitfall of curse of dimensionality but also offers strategic advantage, harnessing strength both dimensionality reduction and clustering



APPlication of unsupervised learining: 
Data visualization: how make sense of data four,5,thousand dimension
Challenge of direct visualization:inherent limit our perception.we live perceive in world in 3D. data resides in spaces 3d cant directly visualized.
Projecting high dimensional data: all not lost some like PCA or t-SNE allow project high dimensional data spaces onto more 
manageable dimensions, two or three.doing so we reduce data dimens by focusing most dominent feature, distilling essence data soemthing visually digest.



What are residuals in cluster analysis?
cluster analysis group similar data point into cluter based on distance blw point within each cluster(intra cluster similarity)
group have high intra- cluster similarity and low inter-cluster similarity. not all data point fit neatly into these clusters
dot not aligh well with particular cluster consider outliers,which might be noise that can removed from your data set to clean it up. outlier might point anomalous behavior and could 

Why might these outliers be considerd noise?
Measurement errors: 
Random fluctuations:
Genuine outliers:

Impact of noise on analysis
Misclassification of data points
Erroneous interpretations
Less effective models or predictions

Case for removing noise
Enhanced clarity:
Improved accuracy:
Streamline data:


Anomaly detection: 
Understanding anomalies in cluster analysis: anomalies or outlier deviate from patterns established by other dataset members.
cluster analysis focus on grouping similar data point into cluster always possiblity that certain data point don't fit neatly

Why are anomalies significant?
anomalies are important for
Signs of fraud:
system failures:
Rare events:

Value of detecting anomalies
carrie with host of benefits
prevention an dquick action:
Insight into rare events:
Optimization:


Dual role cluster analysis
cluster analysis play dual role
Pattern recognition:
Anomaly detection:


Unsupervised learning as feature engineering: create feature engineering
uspervised learning inputs called representaion learing 


DImension reduction:reducing dim making more manageable and enahancing model performance,
Deep learnin:model particularly neural networks excellent at carving out features independently form pile of data
These model can condense large amount information with multiple layer, effectively performing
Clustering analysis: An unsung hero feature engineering domain, cluster analysis aid crafting categorical features.
Every observation your dataset can labeled based on cluster align with. this labeling powerful featurefor supervised learning


Challenge of unsupervised learning:

Clustering performance evaluation:Measures when ground truth labels are available
Adjusted rank index(ARI):similarity of two assignments, ignoring permutation and chance normalization.ARI score 1 perfect clustering
Normalized mutual information(NMI): metric measures mutual information blw ground truth and cluster assignments,normalized esnure value 0 and 1 Higher NMI better clustering
Flowlkes-Mallow Index: pair based approach,considering pairs point same different cluster predictd true labels

Measure when no ground-truth labels are available:
Silhouette coefficient:metric calculates how close each point one cluster point neighboring cluster. It values range from -1 for incorrect clustering to +1 for highly dense clustering with 0 indicating overlapping clusters.
Davies-Bouldin index: similarity ratio blw each cluster and most similar cluster lower distance better clustering
Calinski Harabasz index: score ratio sum blw cluster dspersion within cluster dispersion. Higher values ratio better clustering

Number of clusters or dimensions
Automated selection by algorithms:some algo as DBSCAN(for clustering), inherently decide optimal number clusters or dimensions.

User defined number: K-means for clustering, require users to specify number clusters. using PCA for dimensionality reduction,user can decide no principal components they wish to retain.


Computational complexity: 
Data volume,complexity and computational challenges;unsupervised learing work without label data, demands sustantial volume input data to draw meaningful insights, increase computational demand or costs, power memory usage and time 
Intervention and interpretation:unsupervised learing operate without explicit guidance,far from set-it and forget-itapproach. result produces cluster or reduce dimension
require meticulous interpretation.
Paradox of unsupervised learning: unsupervised paint picture complete autonomy, but reality is nuanced.irony unsupervised learing oftern necessitates discerining human eye to oversee,
guid, and interpret its outcomes while free labeled data constraints. 
unsupervised doesnt signify lack human involvement but rather shift in where and how involvement mainifests.

Iterative Nature
subjectivity of clustering: at heart of unsupervised learing lies attempt group or classify data based on intrinsic patterns. grouping seldom absolute. perfect cluster might not seem 
ideal tommorrow when faced new data or fresh insight. inherent subjectivity implieis clustering result often must reassessed revised, refined multiple iteration.
Balancing act of dimensionality reduction: Reducing diem of data akin compressing detailed novel into short story. alway risk losing critical information
Challenge of expectation vs.outcomes: prior research might suggest specific number of clusters. However,unsupervised algorithms might return different number. Such discrepancies can thought provoking.
Data quality and selection:usupervised learing is only good as data it consumes anomalies, biases, or inappropriate data selection can skew results.Regularly reevaluating data quality and appropriateness ensures result remain relevant accurate ex




Deep learning:
Part of mL ai focus training artificial neural networks perform task withou explict programming. involve use deep neural networks are capable of automatically learning hierarchical representaion

Deep learning refers to deep neural network hvae layer of artificail neurons.

Neural networks: algo inspired by structure and function of human brain they fundamental component of ml and ai


Application deep learning: image recogniton, NLP speech recognition, adept pattern recognition and forecasting
Medical diagnoses:pivotal diagnosing diseases by classifying medical images helping professional identify anomalies.
Marketing strategies: companie use deep learing refine strategies by filtering social network data and analyzing user behavior, ensuring adverisement reach most redeptive audiences.
Financial forcasts: use deep ler make prediction about financial assets such as stocks, bonds, commodities,by analyzing patterns and trends observer historical findan
Enery demand: forecasting electrical load and enery demand. Utility companies manage grids more efficiently by managing supply acc predicted enery demand


Neural Networks:
Rosenblatt invented perceptron,inspired by neuron in brain,task input weight sum input features. output binary result based on magnitude of input.said activated switched on if input sum breaks threshold.

Activation function: whether neuron should activate based on input receives.
Bais: value added activation function response.

Neurons: processing unit in neural network,nodem, perform calculation,produce output
Layers: consist neurons,layers are stacked together from neural,each layer receives input from previous layer,applies math operation,pass result next layer
Weights and Biases: value assign each neuron within layer,determine importance of output,weight are adjusted during training process improve network performance
Activation function:non-linearity in network, allowing to capture complex patterns,they transformations or mathematical operation,applied to input neuron.

Feedforward and Backprogation:
Feedforward:process moving data through network from input layer to output layer.
advanced feedforward model called recurrent neural networks(RNNs)

Backpropagation: algo based on differential calculus that used to adjust neural networks weight biases improve performance.rate of change,error with respect to each weight assign to network
update each by scaling factor model parameter known network learning rate
lr dictates how large step take direction of gradient,smaller step batter chance finding optimal solution
BP start by measuring error(diff blw network output and true label)

**************
Activity:question:qize
role of layer
Neural network:qize: activity



************
Generative AI:model learn from input data use what it learn to generate new data that mimics data learned from
model learn patterns and structures from existing data.
ex-image generation,text generation and data synthesis.
GenAI content generation,data augmentation, data synthesis task, benefiting art, media, ml fields.(1950-60 rim;e base gemAI began,1990 neural network promise generating sequences of patterns,2000 revival deep learning,leading intro generative adverdarial networks 2014 advancing genAI,2010 witnessed emergence of LLM

Application GENai
Text generating: textual data,as sentence paragraphs, documensts,text data generation,human like text,artical,answer questions,engage in NL prominent ex-GPT3 and BERT.
Image processing:Deal with visual data,as photographs drawings,diagrams,generate image,perform image-to-image translation,turning sketches into realistc images, assist object recognition and image captioning tasks ex-DALL-E and BigGAN.
Coding:GenAi focus on generating,editing,and optimizing code, enhance code quality,improve software development process
Audio processing:train audio data generate,manipulate,process sound.voice synthesis,text-to-speech,music generation,speech recognition,noise reduction,audio modality ex-WaveGAN and Tacotron.
Molecular designing:working chemical structures and molecular data,practical application in drug ,material science,chemistry-related research.Reduce need for extensive laboratory 
Video processing:video genaI focus video sequences,task like video gen, style transger in video , action recognition.essential computer vision video processing.

Attention mechanism:enable neural network to pay attention word token within a learned context. allow models focus on specific part of input data,emphasizing relevant information while ignoring less important details.
Transformers: neural networks that learn context and understanding from sequences from words.
Transformer have NLP ,powering machine translation,text summarization,sentiment analysis,chatbots.Noteble model GPT3 and BERT built on transformer architecure
further transformer expanded into computer vision,making strides image classification,object detection and image generation.

Transfer learning with pre-trained transformer model become cornerstone of AI development.Initially trained on vast dataset, these models are fine-tuned for specific tasks.proficiency 
oin languare understanding,including context,idiomatic expressions,nuanced languge usage

GenAI and Large Language models:
development of transfomers came ability to train neural nets on large datasets.Most notably LLm are trained on large dataset,including entire wikipedia corpus.LLM have also train massive data set obtained by web crawlers.
Content on web can be inaccurate,unreliable,and biased.this present big challenge to these model capablity to validate artificailly generated output 

step 1 pretraining on data: Genai LLM pre-trained on extensive datasets containing range of text form internet.During this phase model learn grammar,syntax,semantics,facts,even some level of reasoning by predicting next word in sentence or filling gaps in text
step 2 Fine-tuning fro specific tasks:pre-training LLMs are fine-tuned on specific tasks or domains.ex- fine tuned task like language translation,text summarization question answering and content generation.
step 3 contextual understanding:LLM analyze context of given text prompt and generate content that fits within context.enables them to generate coherent contextually relevant response
step 4 Content generation:LLm generate content,including articles,poetry,stories,code.they emulate different writing styles,adapt different tones,produce content in multiple languages.
Step 5 Application: LLM have practical applicaton in content creation,chatbots,virtual assistants,automaic text generatio,creative writing assistance.Task as text completion,comtent summarization,content translation.


Career management LLM
Does intersection of linguistics and artificial intelligence fascinate you? LLM model engineer career be pergect path for you.
Hands-on experience:project,internship ml and NLP
Certificate like Tensorflow and ai certificate
Read research papers attend conference,participate online forums stay upto date latest advancements.
Engage with AI community through networking,online platform,collaborative projcts,stay informed industry trends


Alignment with current skill and interests: Have you identified how your current skill and interest align with requiremenst of being llm engineer.(excite me natural language processing)
Understanding of challenges:Have you realistically assessed potential challenges and thought about how you might overcome them?(mastering complex, staying updated rapidly)
Awareness of industry developments: Does your research into recent advancements reflect understanding current and future trends in AI and large language model(Have you identified why these development are significant to field and your career aspirations)
Engagement with learning opportunities: Are event workshop you have identified relevant and beneficial for your career path(How you plan leverage these opportunities for networking,learning,and professional growth.
Long-term career planning:Does your reflection show awareness of long-term commitments and continuous learning required in this field?(Have you considered how you will adapt as AI and languge modeling)

Key concepts
lesson 1 Deep learning:
AI simulates human intelligence machine perform task that typically require human intelligence
ML subse AI learn from data without explict programming.
Deep learning(DL) subset ML use neural network architecture to learn from data indepth
AI application range from simple tasks like filtering emails to complex ones like autonomous driving
Ethical consideration AI involve ensuring fairness,transparency,accountability,and privacy

Neural networks
Neural networks computational model inspired by human brain,used extensively AI pattern recognition decision making
consist of input hidden and output layers with hidden layers tranforming signal from input to somethng output layer uses
perceptron is early neural network model that outputs binary result based on input features and threshold
Modern neural networks use multiple layers and complex architectures,like cnn to process data.
Weight and biases are critical in neural networks,determining importance input adn adjusting neuron activation.
Activation function neuron introduce non-linerity, allowing network capture complex patterns.
Backpropagation method used for adjusting networkd weight and biases improve performance by learning form errors.
Neural networks learn form label data in supervised learning scenarios perform tasks like image and speech recognition.


Generatine AI
Gen AI refer system that generate new content mimicking human creativity.
GenAI model learn form large datasets to produce new data instances like images,text,audio,more
Development GenAI ranges from early rule-based system to advanced neural networkd and LLM
GenAI application span text generation, image processing,coding,audio processing,molecular design,video processing
GenAI model like GAN and teansformers rvolutionized creative and analytical tasks
attention machanism tranformer allow genAI focus on relevant parts of data, improving relevance and coherence of generated content.
GenAI made signigicant strides in NLP influencing various application beyond text generation
LLM likegpt3 have trained massive data allowing generate human like text various task and industries.
LLM in GenAI undergo pre-training on large data and fine-tuning for specific tasks,contributing to their versatility.
Ethical consideration vital in GenAI developemt to address biases and ensure the responsible use gen tech


Critical thinking:
Identify ethical consideration in AI to address biases and ensure the responsible use of generative tech
Explain how neural network used for pattern recogniton and decision making
Describe how LLM generate human-like text acros various task and industries
Summarize how GenAI model learn form lareg data produce new data instances

Written communication
Identify step align your current skill and interest and explain why
Enage with AI community throuvh networking events,online platforms and collaborative projects
Describe potential career challenge and strategies overcome them
Explore upcoming AI or ML event workshop online



Personal assistant,autonomous vehicales,smart cities,smart homes,recommendation system,virtual shopping 
human like intelligence resoning abilities been part of human folklore,mythology and speculative fiction for millennia

AI form computer technology mimics human intelligence.

NLP field of ai focus on interaction blw computer and human through natural language.

step 1 Breaking down sentence(box of legos)NLP takes sentence break into pieces word (Tokenization)
step 2 Understanding meaning: figure otu each word( righ mean near bat)
step 3 Grammar and structure: NLP tries label each word figures out word main subject help NLP understand role each word.
step 4 Finding relationships: NLP look how word relate each other in sentence ex-Jenny gives Bob apple,bob receving ,apple what;s being given.
step 5 Getting big picture: uderstanding smaller part NLP tries get main idea or message sentence building picture out of those Lego pieces.
step 6 Learning from ex- NLP system often learn form tons of text ex-more they read ,better they get understanding.it like how we become better readers by reading more books.
step 7 Making adjustments:NLP might not get things right but when corrected,learn from mistakes.over time becomes smarter and makes fewer mistakes.


speech recognition: tech translates spoken language into written text or commandsl
Step 1 Hearing sounds:imagine using giant funnel to catch raindrops. computer use microphone catch sound voice change into digital format.
step 2 Segmenting speech:Just separate string of pearls in individual pearls, computer divides the continuous sound speech into small bits slicing.
step 3 Identifying patterns: matches familiar sounds in its library. having a puzzle and computer tries to find piece fits best.
step 4 Using context for clarity:Read can be past or present tense.checking surrounding words,speech reco can better guess intended meaning
step 5 Translating to text: decingin on probable words , it write them out or acts on spoken command.It like having someone jot down what you are saying
step 6 Learng through listening: Speech recognition system improve by hearing vast amount spoken words. more listen better they become, just as we become better listerners the more we hear and undrstand.
step 7 self-correction: speech recogniton might mishear words. corrected ,it adapts and learns.with time, it gets more accurate efficient.


Assistant tool:
Automated tasks:
predicting needs:
Seeing and understanding:
creatin content:
Having conversations:
connecting to other apps:

Task:scheduling appointments or managing emails
personal relevance:student assistant organize study schedule and track assignment due dates.
iMporve efficency:help filter and prioritize,ensuring respond to most important messages first.

autonomous vehicles: dreamed sitting in car, relaxing with book,letting drive itself
perception:see around make decision mimic human driver.think like car eye and ears.using tool LiDAR,radar and cameras,car paints picture of world make quick decision ensure passenger safety.
Sensor fusion:car brain take information form eyes and ears get clar picture
Localization:car knowing exactly where ai uses GPS and maps to ensure car on right path
Path planning:car decides route, considering traffic roadwork.
control systems:car hands on wheel and foot on pedals.ai control fast car goes where turns

Additional feature in autonomous vehicle
Traffic smarts
pedestrian safety:
Parking:
Delivery bots:
Easy rideshares:
First aid wheels:
AI buses

Attention analytic thinking


AI smart cities and smart homes
smart cities: leverage tech enhance infranstructure,services and overall quality of life
integrate data from various sources improve efficiency,sustainabilty, and citizen engagement.
Components include transportation,efficient energy management,advanced healthcare system enhanced public services.

Smart homes: residence equipped with devices and system can controlled remotely automated increased comfort and convenience.
feature interconnected appliances,lighting,heating,security ,entertainment,optimize energy consuption
iOt play significant role enabling device communicate coordinate actions.

IOT system everyday objects have sensors and connected internet allowing share data.

Device with sensors:
Connectivity:
Data processing:
Action:
User interface:
Cloud and data storage:
Security and privacy:

Big data analytics:process of examing vast and varied dataset to uncover information such pattern correlation,trends,insights.

Enhanced efficiency: smart cities effiectively allocate resources like energy,water,transport,minimizing waste upping efficiency.reduce congestion,saving time and lowering emission. smart homes manage energy.
Improved quality of life: smart home offer convenience,from precooled room hot days personalized entertainment systme
Advanced sustainable practices:smart cities and homes champion green practices. cities adopt efficient transport system waste solution,reducing carbon footprint,enery saving appliance green energy decreasing env impact of urban living.

Data privacy and security:
Infranstructure costs:
Technological inclusion:

Case study:Liam


Generative AI learn from data generate new content ideas,solution were never explicitly programmed into.

Language:GenAI differ form AI that doesn't just follow set rules or patterns.instead,learn create text,image,music,more show human like creativity. make work easier boost creativity many job.
this save time and make thier work better.GEnai understand context, answer in way that makes sense,creative like human,leading many new helpful uses.
Vision: Genai vision focuses on producing realistic visual content reflects real world.algo use tech learn form existing visual data,enabling them create fresh and unique imagery.
ex-crafting physical prototypes fashion designers can now use generative AI quickly generate a plethora virtual models,experimenting various colors,patterns,styles significantly less time

Recommendation system and virtual shopping assistants:
Digital personal shoppers, recommendation systems,use generative ai filter suggestion and advanced algorithm to analyze user perference deliver personalized product or content suggenstion
enhancing online shopping experience and boosting engageement,sales,customer loyalty.

GEnAI reshapes industries,enhance creativity and improve uer experience by generating new unique content.leran from dat,mimics humancreativity,excels various applications,customer service and design healthcare.
Genai seamlessly integrates into worlplace and daily life,revolutionizing content creation,visual design,online shopping, ge
GEnai language,vision,e-commerce virtual assistants.

Field:marketing,journalism,architecture,healthcare,fashion design.
Challenge and needs:what aspects work time consuming,require extensive creativity,could enhanced with personalized automated solution. 
ex-content marketing,creating diverse and engaging content consistently challenging and time consuming.
GenAI application:how genai could specifically address challenges and needs.identify particular functionalites of genAI 
ex-genai can assist content maketer by generating variety of content ideas,creating drafts,even generating entire article

challenge and consideration of genai language.
Content accuracy and quality control
Addressing bias in Ai generated content.
Mitigating disinformation risks

General adversarial network: pair of neural networks,generator and dicriminator,which trained simultaneously through adversarial training.

GAN
style transfer:
Medical image analysis:
Drug discovery:
video game graphics:
Special effects in movies:
Fraud detection:
Simulating data experiments:
Art generation:
Music composition:
Customer experience:

Variational autoencoders(VAE):neural network focus on representing inptut data concise and informative codes in latent space and reconstructing data bake from these codes.
principles probability and statistical ingerence,using encoder trarnform 

VAE:
Image genearation:
Image denoising:
Style transfer:
Anomaly detection:
Data compression:
Recommender systems:
Drug discovery
Financial modeling:
Financal recognition
content generation

Conditional generation:process generating data that not only realistic but also adheres particular attribute or condition set by user
personalized content creation
product design
Advertising
Video game development
Fashion industry
Film production
User interface design
Architecture
Training simulation
Healthcare:


Challenge GenAI
Deep fakes and authenticity challenges:
Navigating ethical dilemmas in visual AI:
Legal implications and regulatory responses:
Promoting media literacy and public awareness:


Recommendation sysetems
Virtual shopping assistant:use NLP,visual search,augmented reality,personalization guide.
Ex-understand user request in natural language,find products using image search ,users visulize product,space with AR.

Recommendation system versus virtual shopping assistant.


Case study: shopSmart Inc.

Integration AI across various fields:marketing,journalism,architecutre,healthcare,fashion design.
job:whether usderstanding ai and functionalities

Explore AI application develop better undrstanding of different fields that AI use 
how they work together, you will explore city map using areas to explore how ai can user in each location

HOMe:AI enhance security,convenience,enery efficiency.smart thermostats like nest use AI learn homeowners preference over time,adjusting temp automatic
smart device play music,provide information and assist with tasks analyse video footage realtime detect unusual activity promptly alert homeowners
Traffic camera:
Vehicle:
Home office:
office building1:
office building 2



AI world today:

Impact of AI on Business:

Impact of AI on Customer experience:

Customer experience:includes every brand,initial awareness to post-purchase support.It not just about transaction,about building meaningful relationships foster loyalty and advocacy.

Starbucks:
step 1
step 2


Customer analytics: process collecting,processing,analyzing,interpreting customer data,past pruchases,browsing history,social media activity.
Demographic info: age,gender,location,enables business understand customer attributes.data aids tailoring marketing strategies,such as clothing retailer using age gender for personalized fashion suggedtion.
Transactional information:tracks customer purchases history interaction,allo


Impact of AI on informed Decision Making:

Impact of AI on risk management and Human Resources

Risk management:process identifying assessing,controlling threats organization capital and earning.These risk can stem from variety of sources,inncluding financial undertainties,legal liabilities,strategic management errors,accidents and natural disasters.
step 1 Identifying risks:risk management involves recognizing potential risks could negatively affect organization.like physical risk natural disaster damaging),abstract risk fluctuation impacting financial positions.
step 2 Assessing risks: evaluated understand potential severity likelihood occurrence.assessment help prioritizing risks,hight-impact,high-probability risk,minor effects.
step 3 Controlling risk:establish strategie manage risk.involve transferring risk(insurance),avoiding risk,consequences particular risk.

Airline and risk management
Identify risks:associated with flight operations,including potential delays and cancellation lead customer dissatisfaction,additional costs,logistical challenge.
Assessing risks:analyzing historical data on weather patterns,mechanical reliability different models,and air traffic trens.might find flight certain destination 
Controlling risks:
Weather risk mitigation:monitor weather forecasts contingency plan,rerouting flights scheduling earlier departures avoid bad weather.
Maintenance schedules:employ regular and thorough maintenance aircraft minimize failures.stock spare part key airports quickly address mechanical issures
Overbooking strategies:empty seats caused last-minute cancellation airline use overbooking strategies
Customer service policies:flexible rebooking compensation policies affected passengers manintain customer satisfaction loyalty.
Risk monitoring and review:continuously monitor effectiveness,strategies and adjust based on ongoing  data analysis changing condition.



ChatFPT,Bard,ai assistants,co-pilots,human augmentation(cretiv ewiriting,productivity,video scripts,data analysis,cading summery generation,marketing research, knowlede discorery with Rag,customer services,research assistant
stable diffusion,Dall-E(generative art work,Rich images form text gaeming and video
Creative expression(GentAI)

Intelligence ability ro acquire and apply knowledge in useful ways.
working:ability fo cs to acquire and apply knowlede useful ways
learn,axquire and generalize

learn fuctions-essential task of ai learn fun and aply them useful way
fun -take input give output
simple function: image classification:classification,semantic segmentation,leran form image to categories
Object dection-fr function from image to set of boxees and categories
Face recognition-function form face images to semantic embeddings
Chatfpt- function from text tp mext wprd
Semantic segmentation-function from imgae pixels

function-represen tfun as nerral nets:neural nets universal function
learning task if planet position y known at time x 0,1,2,3 and 4 day 
learning process-shown manually,-increase w,if loss goes down -ve gradient,keep new w,if loss goes up + gradient ,roll back increaseand then decrease w by small amount,above
if gradient + you tdecrease param.this gradient descent,
learnin grate control how much you change param

Training neural nets(learn function-big picture
via gradient descent,Randomly initialize parameters/weithts,feed data forward,ocompute loss,foorward pass compute output,compute loss,Determine how strongly weitht contribute loss i.e gradient,backward pass;backprop compute gradient,update weights based how strongly contribute loss ie gradient,in manner reduce loss,use leaning rate for small step right direction,

val model = new Model 
model.train(xdata,ydata)
val ypreds = model.predict(

Aside: Array and Tensor(NDA arrays)
array for sequntial data,neural nets, use NDAarrays,benefits,eff9ece9jt memory layou,aribitrary number dimesn,support vectrized,more numericlal ops,work on gpu: Rank0 scaler,Rank 1 vecotor,Rank 2 matrix:rank tells number of axes,each axess have any number of dimension:axiss dimension(rank)vs feature dimn:shape of tensor given you ingo shape 3,4,4:total(feature)dimensionality of tenor

Aside-NDArrays:Rank 1 alway sbatch of N to fully utilize hardware.RRank2 tabular ot tim eseries data,shape(N,F),Rank3-grayscale image,shape(N,H<w)rank4-gb image,shape(N,C,H,W) Rank5 volumetric medical data shape(N<CmD,H,W)

Model Training:create shape, weight and biasss
modelfunctiox.matMul(w).add(b)
epoch< 1 to 1000,gc,ypred,loss,backward(loss)
params.foreach 

Model prediction: predict(xva: Array y.toFloatArray,nm.create(xvalue).reshape

Learning Arbitrary func:N neuraon layer can learn poly with m bend or n+1:anycurve appprozimately with polyline n bends(sufficient large n)
much more efficient have multiple layers neurons.L layers with n neurons each learn lines with (n pow L) bends.where deep learning
