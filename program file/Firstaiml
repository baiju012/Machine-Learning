linear regression : statistical algorithm that model relationship between dependent and independent variables. linear regression finds best-fitting linear equation predicts dependent or target variables
Ordinary least squares most common method used in linear regression aim to minimize error blw observed data point and predicted values(Ridge regression-imporve performance of lr
, Lasso regression->increase model interpretation, Elastic net combines ridge and lasso, Non negative least squares regression variant-> making ideal for Non negative values such as price or temp

Logistic regression: Binary classification problems. refer class or category, relationship blw independent variables and probability of observation belonging to particular class.it applies a user defined threshold(hyperparameter)
Logistic used for decision making data into two classes.such as whether to invest in certain stock,buy or sell a 
business or make medical diagnosis.



Decision trees :provides way make systematic decisions using flowchart like set binary decisions. It is hierarchical,branching tree like structure 
if condition1 and condition2 ... and condition3 then outcome
Decision trees used both classification and regression tasks. making decission at each node
starting with labeled data and potential features for branching. divided into subsets hold specific feature sets. 
ex-gini metric lower gini value higher quality.
limitation DT : can overly complex, leading to overfitting. 
overfitting can controlled through methods such as tree pruning, setting a minimum sample count for leaf nodes or limiting the tree's depth

Support vecotor machines(SVMs): a supervised ml algo use classification and regression task.operates on principle of finding a hyperplane or line in two -dimension space that separates two classes of data points.
discriminative classifier solve both classification and regression problems as supervised algorithm, svm requires labeled data, model train on subset data 
SVM classifier are well known for their abiltiy to identify hand-written digits
use for facial recoginition, image segmentation(differentiating organs or tssues in MRI) life expectancy, and expected return on investment(ROI) in financial industry
Linear SVM: main objective to find line or plane that best separates classes in target variable from each other.
Kernel SVM: Many real world problem cannot divided into classes by simple line.svm applied using "kernel trick" uses curved lines insted of strainght lines to separate the class.iris use linear kernel and linearsvm
SVM can high accuracy, working well with high-dimensional data, low sensitivity outliers.

K-nearest neighbors(KNN) : train data based on label target classes. algo can conceivably use lidar images distinguish 3D human and 2D pictures
KNN- prevalent in many sectors, such as healthcare(for disease prediction), finance(credit scoring), and e-commerce(for rcommendation systems)
KNN classification (predict categories): uses knn algotithm to categorize labeled data point by analyzing the categories labeled data point by analyzing categories of K nearest neighbors in feature space 
KNN classification has seen widespred use in image recognition where image classified based on feature of similar images in dataset.
voice recoginition system use knn to classify voice command by comparing them to known voice patterns
KNN classify genes into dfferent functional cateories in bilogical sciences and genetics.

KNN regression: class becomes very large(continous value)
real estate: predict price of house based on feature(area, nu bedroom, location) knn reggression used averageing the prices of Kmost similar house
stock market forcasting: historical stock price movements can inform future price predictions
Weithted averaging: closer neighbors can give more weight in averaging process


EValuation matrics for classification
Accuracy: straightforward metric represent ratio of correctly predcted instances total instances in dataset. beneficail when class distribution is blanced 
in case class is imbalance accuracy can misleading for instance, 95% samples belong to classA and 5% belong to classB model predicting everything as classA still accuracy 95%

Precision and recall: metric measures accuracy of positive prediction wile recall measures the ration of positive instance 
precision and recall especially important when false positive and false negatives carry different costs.for flase negative(disease present but not detected) more dangerous in medical test than false positive(disease absent but indicated as present)

Confusion matrix : is table used to describe performance of classification model on set of data true values known four value TP,FP,TN,FN 
true positives and true negatives indicate correct classifications, where false postivies and false negatives indicate errors, ture positive would disease present and detected while true negative would be disease absent and not detected as present


Area Under the curve-receiver operating characteristic(AUC-ROC): curve plots true positive rate(recall) against false positive rate
AUC represent area under ROC give scalar value, indicates model ability to distinguish between positive and negtive class
AUC-ROC useful dealing with dataset with class imabalance. AUC value 0.5 suggests no discrimination(equivalent to random guessing) 
value 1 indicates perfect discrimination. ex- credit card transaction most lgitimate very few fraudulent, which represents imabalance two transaction classes.


Evaluation matrics for regression: examine relation blw two or more variable of interest.
R-squared(R2): coeffieient determination, quantifies proportion of variance in dependent variable that predictable from independent variables 
range(0-1) 1 INDICates regression predictions perfectly fit data R2 suggest better fit, doesn't always mean better model, especially if model is overfitting.
Mean squared error(MSE): av squared differences blw observed and predicted values.MSE emphasize more significant errors over smaller ones
since squares residuals.means sensitive to outliers lower mse generally consider better
Mean absolute error(MAE): calcualte avg absolute difference blw observed and predicted values
MAE treates all errors equally, so it is less sensitive to outliers compared to Mse similar to MSE a lower MAE indicates better model fit data



Natuaral Language Processing (NLP):bridge blw intricate nuances of human language and analytical prowess data science.
NLP use AI focuses on teaching computers to understand,intrpret, and interact with human language way that both meaningful and useful. 
Computer science: programmer design and develop NLP algotithms to efficeintly process and analyze vast volumes of text data.these algo enable machine to perform text classification, sentimetn analysis and Language translation task
Liguistics: linguistics, study of languages, brings valuable insights to NLP , programmers developers use these insights to help NLP system understand and work with text and speech data
Machine learning: programmer use ml to train deep learning models to learn and adapt language patterns.NLP-driven model have signigicantly advanced text understanding and response
Data science: collecting, analyzing preprocessing, and structuring vast amount ot textual dat for analysis. NLP leverages data science technique to train model derive insights from textdat

Practical application and task NLP
Text understanding: NLP systems use text understanding analyze and intrpret meaning of textual data.
Language translation: NLP use convert text or speech from one language to another. NLP powwered translation system apply advanced techniques to ensure accuracy and fluency translated content, helping people from different linguistic backgrounds communicate.
Sentiment analysis: application of NLP help business analyze customer sentiment and opinion expressed text data such as social media post, review or comments.allow businesses to understand whether customer express positive negative aor neutral sentiment
Speech recognition: NLP converts spoken language into text, process widely used voice assistants and transcription services. NLP also improves accessibility individuals with disabliltes


two spect NLP 
Natural language understanding(NLU):enable computer to understand interpret human language.It involves task as text comprehension, language modeling,sentiment analysis and recoginition. NLU allow NLP systems to grasp meaning, context  and behind text or speech
Natural language generation(NLG): focuses on generation of human like text or speech by computers. NLG systems transform structuted data into coherent and contextually relevant languages.
this use various application including text summarization, language translation, chatbots, voice assistants, and personalized email to enable machine to speak to humans and natuarally and understandably

Common programs using NLP
Microsoft word: nlp feature like spell check, grammar check, and autocorrection. It also offers suggestion for improved sentence structure and readabilty
Google search: engine use NLP algorithm to understand user queries and provide relevant search results. Feature like google autocomplete and featured snippets also use nLP
social media platforms: like facebook and linkedin use NLP for content recommendation, sentiment analysis and trending topic identification.
voice assistants:like apple siri,amazon's alexa, google assistant rely on NLP understand and respond to spoken language commmand and questions.
Customer support chatbots: service use chatbot nLP capabilities provide instant customer support, answer frequently asked questions and assist users with inquiries.
Content recommendation systems: Streaming platforms like Netflix and music service like spotify use NLP for content recommendation, suggesting movies, shows, or music base on preferences
Healthcare information systems: NLP usd heathcare information system process medical records extract patient information and assist in clinical decision
Virtual learning environments: automated grading ,personalized course recomendation nlp.

Ambiguity: ambiguous, word and phrases have multiple meaning and association depending on contect how the word is used. NLP models use contextual information and surrounding words to disambguate.
Lack of context: struggle with maintaining context in long conversation or understanding subtle nuancess. Models with memory mechanisms, like transformers, help capture context over longer text
Bias and fairness: model inherit biases from training data, leading to biased or unfair outcomes.Addressing bias requries diverse and representative training data,ongoing monitoring, fairness-aware model design mitigate and correct biases.


Tool and techniques include NLP
Tokenization: Breaks text into individual words or tokens,forming basis for most NLP tasks.
Stemming and lemmatization: words to root forms,aiding text analysis and rducing vocabulary variations.
Part of speech tagging(POS): grammatical labels to words,helping syntax analysis and understanding sentence structure
Named entity recognition(NER): categorize named entities,such as names people,place and businesses in text.
Sentiment analysis: tool assesses emotional tone expressed text and often used for sentiment classification,such as positive,negative or neutral.
Text classification:text into predefined classses or labels,such as spam detection or topic classification
Machine translation:like google translate,NLP techniques translate text blw languages.
Pre-trained language model:Large scale such as google BERT and openAI chatGPT, revolutionized NLP providing deep contectual understanding and generating human like text.


NLP libraries and framework
NLTK : comprehensice python library for NLP, contain many tool and resources task like tokenization,stemming,lemmatization,pos tagging.
spaCy: fast and efficient python library for NLP. provids pre-trained models and supporttask like tokenization,pos tagging, and NER.spaCy known for speed and production-readiness.
Transformers from Hugging Face:Provides state-of-the-art  pre-trained language models shuch as google BERT.It's widely used various NLP task text classification,language generation , translation
Standford NLP:tool and model including tokenization,pos tagging and NER.It known for its high quality models and accuracy ofeten used for academic and research purposes.
AllenNLP: deep learing framework specifically designed for NLP research. provides pre-built components for building and evaluting NLP models

Text acquisition->Text preprocessing_>sentence structure and interpretation_>Numerical representaion->Machine training and accuracy checks->Application development


Tokenization,stemming,lemmatization,pos tagging,NER, sentiment analysis,text classification,machine translation 
NLTK,spaCy,transformers from Hugging Face,Standford NLP ALLenNLP


UNSUPERVISED MACHINE LEARNING:
unsupervised ml similar to learing without a teacher, computer learn exploring data and finding structre and patterns on its own ex spot patteren in customer data
Unlabeled data:don't have known or specified outcome.
usupervised learning: unlabel data uncover hidden patterns or structures in data, unlabeled data makes it valuable in real world scenariosex-text,image,transaction record
it delineate inherent structures,regularities, or recurring thimes within data. these underlying patterns offer valuagle insight knowledge,data compression,subsequent supervised tasks.

Clusters: group similar each other according defined similarity measure ex-
Association learing focus on uncovering patterns or relationships blw seemingly unrelated item occurrences.
Reduced dimensionality:smaller features often extracted from dataset preserves essential information content data reduce number dimension and amount of data

Clustering: such K-means, hierarchical clustering, DBSCAN aim to divide data into distincet groups based similarity.
Dimensionality reduction: as PCA, t-SNE , autoencoders can undcover lower dimensional structures within high dimensional data salient features or patterns.
Association learning: as Apriori or FP-growth used discover items often associated with each other dataset
Anomaly detection: recoginizing normal patterns in data,unsupervised method spot outliers or anomalies deviate from patterns

Advantange unsupervised learning:
Cost effective and scalable: large amount of unlabeled data than labeled data.leverage large data sets extract patterns and insights
Revealing unknown patterns:not being constrained by predefined labels,unsupervise learning unexpected or novel patterns that might not be apparent or considerad
Data preprocessing: recognizing and extracting patterns help cleaning data,imputing missing values,or transformin features 
Avoid bias: no labels guide learning process, less risk of bias form labeling process however,biases data collection or representatin can still impact unsupervise models

two individuals with close ages and income would be considered similar likely grouped into same cluster comparing driving distance ciy manhattan distance metric
usupervised learnig provide quantitative measure to compare data point, enabling algorithm to discover patterns group or reduce dimension dataset gidance of explicit labels

Handling-dimensional data features researchers enconter curse dimensionality.
Curse dimensionality: arise working with high-dimensional data. as number of dimensions(or features) dataset increases, volume of data required perform ml task increase exponentially.

Dimensionality reduction: techniques such as principal component analysis(PCA),t-SNE and autoencoders help reduce number of dimensions while preserving much of data structure as possible
Feature selection:instead using all available features, algorithms or domain expertise employed to select subset most informative and non-redundant feture
Sampling: curse due to overwhelmingly large number of samples across dimensions, techniques such as random sampling or more sophisticated sampling method help reduce computational
Regularization: Techiniques applied penalize complexity of model, making less likely to overfit high dimensional noise.
Using specialized algorithms: algo are designed be adapted work better with high-dimensional data, such random forests or certain kernel methods clustering.

method -clustering method operate different way might partition data into subsets, establish hierarchical trees, or determine clusters based on density.
Dimensionality reduction projection onto lower dimensional space while preserving data.

Scikit-learn :
python library for ml and data analysis. Born from scientific computing ecosystem around pyhton.
It offer array of tools that make process of modeling data simple and efficient. known for user friendly interface, scikit-learn encompasses comprehensive set of algorithms for various task
including dimensionality reduciton
open-source and community-driven; scikit-learn is open source licensed under BSD, developement benifited from the contribution numerous indibidual
Built on NumPy,SciPY and Matplotlib: At core scikt-learn heavily relies on NumPy numerical operation, SciPy certain mathematical functionalities, oftern pair with Matplotlib for visualization
Consistent API:Scikit-learn defining feature consisten API. Regardless type algotithm be classification, regression, clustering model instantiation, training, prediction 
Wide array algorithms: from classic statistical model to more modern ml algo, scikit-learn boasts comprehensive toolkit. 
unsupervised include clustering (KMeans,Hierarchical clustering),dimensionality reduction(PCA,t-SNE), even manifold
Extensibility: Scikt-learn has vast suite method also designed allow users to seamlessly develop and integrate their custom algorithms
Preprocessin and utilities: Scikit-learn provides range utilities for data preprocessing,including tool feature scaling,encoding categorical variables, imputation
Model evaluation and hperparameter tuning: Scikti-learn tool excel for model evaluation and hyperparameter tuning. include cross-validation, grid search, various metrics evaluate model per
Pipline integration:power full features ability create pipeline. ability streamlines many steps model building process, ensuring transformations and estimator executed in correct sequence
Sparse data support: scenarios where data is primarily zeros(like text data represented bag of words format), Scikit-learn can efficiently handle sparse matrices, making more memory efficient.'
Interoperablility: Scikit-learn not handle deep learning tasks, plays well with other libraries. Libraries such as Keras, example integrated with scikit-learn for more complex tasks.


Clustering: data mining technique for grouping unlabeled data based on similarities or difference

K-means clustering:popular unsupervised clustering technique in K-mean clustering,'K' number of user desired clusters for algorithm form from dataset. centroid point represents center each cluster reference point determining distance blw
data pont cluster center first centroid slected randomly fromdata 

Unsupervised KNN: it supervised learning algorithm, can use it unsupervised learning. Unsupervised KNN algo groups togethere data point based on similarity their KNN feature space. idea is if most of your immediate neithbors belong paritcular cluster
DBScan: density-based spatial clustering application with noise. DBScan group point closely packed together based on distance measure and minimum number of point. useful finding clusters of arbitrary shapes instead of Kneasn, which tends to find spherical clusters.
Point in low density regions,neighboring point are too sparse,typically considerd noise or border points

Hierarchical clustering: create tree of clusters. It can either agglomerative(bottom-up approach) or divisive(top-down approach)
in agglomerative hierarchical clustering, each data point starts as its own cluster,and pairs of clusters merge as one moves up hierarchy.
Divisive clustering starts with one cluster,performing split recursively as one moves down the hierarch result dendrogram- tree like diagram

HDBScan: hierarchical density-based spatial clustering of application with noise. extension of DBScan stand out by ability find clusters of varying densities. 
insted using single value density(DBScan),HDBScan constructs hierarchy or clusters dynamically determines best density value cluster formation, making more flexible capable of detecting nested or compounded clusters.


Dimensionality reduction:tackle challenge transforming original high-dimensional data into lower dimensional space,intending keep much data possible unchanged.
Ex 2D,3D,1D
2D reduction mthods (linear projection, Manifold projection)

linear projection: subset of dimensionality reduction techniques transform data onto lower dimensional subspace using lingear combinations original feature.

Principal component analysis: PCA use dimensionality reduction,identifies axes(principle components) in data maximize variance.
by projecting data onto axes,PCA retains data's most significant structures and pattern while reducing dimensions.resulting principal components are othogonal each other ensuring capture non redundant information

Independent component analysis(ICA): PCA look for uncorrelated axes independent component analysis(ICA) aims to find statistically independent axes.particularly useful when goal separate multivariate signal additive,independent components, makine popular 
in audio and image processing fiels.ICA often used task such as blind source separation,objective extract individual signals from mix, separating multiple voice speaking simultaneously

Singular value decomposition(SVD): factorization method decomposes matix into three other matrices, capturing essence original data.
in context of dimensionality reduction, SVD trrasnform data into space defined by singular values, which are indicative data inherent structures.
SVD underlying technique behind many other mthods, including PCA and applied areas such as recommendation system in NLP


Manifold projection: finding linear combination of features reduce dimensions, minifold projection method design capture nonlinear structure inherent data. 
They attempt to map high dimensional data to lower dimensional space, maintainging realtive distances blw points,especially close together. idea is preserve geometric and topological properities data manifold offering more intricate sometimes more accurate representation of original data.


T-SNNE(t-distributed stochastic neighbor embedding): popular manifold learnig algorithm that visulize high dimensional data in two or three dimensions. By convering Euclidean distances blw data point conditonal probabilities,t-sne captures both 
local and global structures. it t-distiributed probability measure ensures similar data point modeled by nearby point low dimensional space, making especially 
effective for clustering visualizaiton. can comptationaly intensive and might not always yield consisten results across different runs.


UMAP(uniform mainifold approcimation and projction):
gian popularity for its efficeinecy and versatility.like t-SNE, UMAP focus preserving local structure,
leading consistent interpretable embeddings.foundation toplogical data analysis give robust mathematical grounding, outperforms other techniques in terms of runtime, especially on large data

Combined dimensinlity reduction and clustering:
by applying dimension reduction first potentially rejuvenate performance of clustering algorithm. two step approach- first simplifying data structue then clustering often enhance quality of cluster making more cohesive and bettere separated. t
this combination not only sidestep pitfall of curse of dimensionality but also offers strategic advantage, harnessing strength both dimensionality reduction and clustering



APPlication of unsupervised learining: 
Data visualization: how make sense of data four,5,thousand dimension
Challenge of direct visualization:inherent limit our perception.we live perceive in world in 3D. data resides in spaces 3d cant directly visualized.
Projecting high dimensional data: all not lost some like PCA or t-SNE allow project high dimensional data spaces onto more 
manageable dimensions, two or three.doing so we reduce data dimens by focusing most dominent feature, distilling essence data soemthing visually digest.



What are residuals in cluster analysis?
cluster analysis group similar data point into cluter based on distance blw point within each cluster(intra cluster similarity)
group have high intra- cluster similarity and low inter-cluster similarity. not all data point fit neatly into these clusters
dot not aligh well with particular cluster consider outliers,which might be noise that can removed from your data set to clean it up. outlier might point anomalous behavior and could 

Why might these outliers be considerd noise?
Measurement errors: 
Random fluctuations:
Genuine outliers:

Impact of noise on analysis
Misclassification of data points
Erroneous interpretations
Less effective models or predictions

Case for removing noise
Enhanced clarity:
Improved accuracy:
Streamline data:


Anomaly detection: 
Understanding anomalies in cluster analysis: anomalies or outlier deviate from patterns established by other dataset members.
cluster analysis focus on grouping similar data point into cluster always possiblity that certain data point don't fit neatly

Why are anomalies significant?
anomalies are important for
Signs of fraud:
system failures:
Rare events:

Value of detecting anomalies
carrie with host of benefits
prevention an dquick action:
Insight into rare events:
Optimization:


Dual role cluster analysis
cluster analysis play dual role
Pattern recognition:
Anomaly detection:


Unsupervised learning as feature engineering: create feature engineering
uspervised learning inputs called representaion learing 


DImension reduction:reducing dim making more manageable and enahancing model performance,
Deep learnin:model particularly neural networks excellent at carving out features independently form pile of data
These model can condense large amount information with multiple layer, effectively performing
Clustering analysis: An unsung hero feature engineering domain, cluster analysis aid crafting categorical features.
Every observation your dataset can labeled based on cluster align with. this labeling powerful featurefor supervised learning


Challenge of unsupervised learning:

Clustering performance evaluation:Measures when ground truth labels are available
Adjusted rank index(ARI):similarity of two assignments, ignoring permutation and chance normalization.ARI score 1 perfect clustering
Normalized mutual information(NMI): metric measures mutual information blw ground truth and cluster assignments,normalized esnure value 0 and 1 Higher NMI better clustering
Flowlkes-Mallow Index: pair based approach,considering pairs point same different cluster predictd true labels

Measure when no ground-truth labels are available:
Silhouette coefficient:metric calculates how close each point one cluster point neighboring cluster. It values range from -1 for incorrect clustering to +1 for highly dense clustering with 0 indicating overlapping clusters.
Davies-Bouldin index: similarity ratio blw each cluster and most similar cluster lower distance better clustering
Calinski Harabasz index: score ratio sum blw cluster dspersion within cluster dispersion. Higher values ratio better clustering

Number of clusters or dimensions
Automated selection by algorithms:some algo as DBSCAN(for clustering), inherently decide optimal number clusters or dimensions.

User defined number: K-means for clustering, require users to specify number clusters. using PCA for dimensionality reduction,user can decide no principal components they wish to retain.


Computational complexity: 
Data volume,complexity and computational challenges;unsupervised learing work without label data, demands sustantial volume input data to draw meaningful insights, increase computational demand or costs, power memory usage and time 
Intervention and interpretation:unsupervised learing operate without explicit guidance,far from set-it and forget-itapproach. result produces cluster or reduce dimension
require meticulous interpretation.
Paradox of unsupervised learning: unsupervised paint picture complete autonomy, but reality is nuanced.irony unsupervised learing oftern necessitates discerining human eye to oversee,
guid, and interpret its outcomes while free labeled data constraints. 
unsupervised doesnt signify lack human involvement but rather shift in where and how involvement mainifests.

Iterative Nature
subjectivity of clustering: at heart of unsupervised learing lies attempt group or classify data based on intrinsic patterns. grouping seldom absolute. perfect cluster might not seem 
ideal tommorrow when faced new data or fresh insight. inherent subjectivity implieis clustering result often must reassessed revised, refined multiple iteration.
Balancing act of dimensionality reduction: Reducing diem of data akin compressing detailed novel into short story. alway risk losing critical information
Challenge of expectation vs.outcomes: prior research might suggest specific number of clusters. However,unsupervised algorithms might return different number. Such discrepancies can thought provoking.
Data quality and selection:usupervised learing is only good as data it consumes anomalies, biases, or inappropriate data selection can skew results.Regularly reevaluating data quality and appropriateness ensures result remain relevant accurate ex
