# Import required packages
import copy
import matplotlib.pyplot as plt
import numpy as np
import random
import time
import torch
import torch.nn.functional as F
from tqdm import tqdm

# Create an abstract multi-LoRA model
class AbstractMultiLoraModel(torch.nn.Module):
    def __init__(self):
        super().__init__()
        self.embedding = torch.nn.Embedding(10, 10)
        self.linear = torch.nn.Linear(10, 10)
        self.lm_head = torch.nn.Linear(10, 10)

    def linear_lora(
        self,
        x: torch.Tensor,
        loras_a: torch.Tensor,
        loras_b: torch.Tensor,
        lora_indices: torch.LongTensor,
    ) -> torch.Tensor:
        raise NotImplementedError()

    def forward(self, input_ids, loras_a, loras_b, lora_indices):
        x = self.embedding(input_ids)
        x = self.linear_lora(x, loras_a, loras_b, lora_indices)
        x = self.lm_head(x)
        return x

# Define a loop-based multi-LoRA model
class LoopMultiLoraModel(AbstractMultiLoraModel):
    def linear_lora(
        self,
        x: torch.Tensor,
        loras_a: torch.Tensor,
        loras_b: torch.Tensor,
        lora_indices: torch.LongTensor,
    ) -> torch.Tensor:
        y = self.linear(x)
        for batch_idx, lora_idx in enumerate(lora_indices.numpy()):
            lora_a = loras_a[lora_idx]
            lora_b = loras_b[lora_idx]
            y[batch_idx] += x[batch_idx] @ lora_a @ lora_b
        return y

# Define a gathered multi-LoRA model
class GatheredMultiLoraModel(AbstractMultiLoraModel):
    def linear_lora(
        self,
        x: torch.Tensor,
        loras_a: torch.Tensor,
        loras_b: torch.Tensor,
        lora_indices: torch.LongTensor,
    ) -> torch.Tensor:
        y = self.linear(x)
        lora_a = torch.index_select(loras_a, 0, lora_indices)
        lora_b = torch.index_select(loras_b, 0, lora_indices)
        y += x @ lora_a @ lora_b
        return y

# Benchmark function
def benchmark(model):
    avg_latencies = []
    for bs in range(1, max_batch_size + 1):
        latencies = []
        for _ in range(nsamples):
            input_ids = torch.randint(
                vocab_size, (bs, seq_len), dtype=torch.long)
            lora_indices = torch.randint(
                num_loras, (bs,), dtype=torch.long)

            t0 = time.time()
            next_token = generate_token(
                model,
                input_ids=input_ids,
                loras_a=loras_a,
                loras_b=loras_b,
                lora_indices=lora_indices,
            )
            latencies.append(time.time() - t0)

        latency_s = sum(latencies) / len(latencies)
        avg_latencies.append(latency_s)
        print(bs, latency_s)
    return avg_latencies

# Constants
seq_len = 8
vocab_size = 10
nsamples = 500
max_batch_size = 64

# Create a LoopMultiLoraModel instance
model = LoopMultiLoraModel()

# Benchmark loop-based multi-LoRA model
avg_latencies_loop = benchmark(model)

# Visualize loop-based multi-LoRA latency
x = list(range(1, max_batch_size + 1))
plt.plot(x, avg_latencies_loop, label="loop")

plt.xlabel('Batch Size')
plt.ylabel('Avg Latency (s)')
plt.title('Multi-LoRA latency w.r.t. batch size')
plt.legend()

plt.show()

# Create a GatheredMultiLoraModel instance
model = GatheredMultiLoraModel()

# Benchmark gathered multi-LoRA model
avg_latencies_gathered = benchmark(model)

# Visualize gathered multi-LoRA latency
plt.plot(x, avg_latencies_loop, label="loop")
plt.plot(x, avg_latencies_gathered, label="gathered")

plt.xlabel('Batch Size')
plt.ylabel('Avg Latency (s)')
plt.title('Multi-LoRA latency w.r.t. batch size')
plt.legend()

plt.show()
