# Import required packages and load the LLM
import copy
import matplotlib.pyplot as plt
import numpy as np
import random
import time
import torch
import torch.nn.functional as F
from tqdm import tqdm
from transformers import AutoModelForCausalLM, AutoTokenizer
from transformers.models.gpt2.modeling_gpt2 import GPT2Model
from utils import generate

model_name = "./models/gpt2"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)

# Define PAD Token = EOS Token = 50256
tokenizer.pad_token = tokenizer.eos_token
model.config.pad_token_id = model.config.eos_token_id

# pad on the left so we can append new tokens on the right
tokenizer.padding_side = "left"
tokenizer.truncation_side = "left"

# Define a Float 32 type
def get_float32_dtype(self):
    return torch.float32

GPT2Model.dtype = property(get_float32_dtype)
model.get_memory_footprint()

# Define a quantization function
def quantize(t):
    min_val, max_val = t.min(), t.max()
    
    scale = (max_val - min_val) / 255
    zero_point = min_val

    t_quant = (t - zero_point) / scale
    t_quant = torch.clamp(t_quant, min=0, max=255)

    state = (scale, zero_point)

    t_quant = t_quant.type(torch.uint8)
    return t_quant, state

t = model.transformer.h[0].attn.c_attn.weight.data
t_q, state = quantize(t)

# Define a dequantization function
def dequantize(t, state):
    scale, zero_point = state
    return t.to(torch.float32) * scale + zero_point

t_rev = dequantize(t_q, state)
torch.abs(t - t_rev)

response_expected = generate(
    model,
    tokenizer,
    [("The quick brown fox jumped over the", 10)]
)[0]
response_expected

# Apply the quantization technique to the entire model
def quantize_model(model):
    states = {}
    for name, param in model.named_parameters():
        param.requires_grad = False
        param.data, state = quantize(param.data)
        states[name] = state
    return model, states

quant_model, states = quantize_model(model)
quant_model.get_memory_footprint()

def size_in_bytes(t):
    return t.numel() * t.element_size()

sum([
    size_in_bytes(v[0]) + size_in_bytes(v[1])
    for v in states.values()
])

def dequantize_model(model, states):
    for name, param in model.named_parameters():
        state = states[name]
        param.data = dequantize(param.data, state)
    return model

dequant_model = dequantize_model(quant_model, states)
dequant_model.get_memory_footprint()

response_expected = generate(
    dequant_model,
    tokenizer,
    [("The quick brown fox jumped over the", 10)]
)[0]
response_expected
